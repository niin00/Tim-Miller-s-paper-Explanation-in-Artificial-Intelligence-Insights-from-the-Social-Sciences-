# Explanation in Artificial Intelligence: Insights from the Social Sciences

**Author**: Ninad Hagi  
**Date**: April 2025 

## Abstract  
Explainable Artificial Intelligence (XAI) has gained renewed interest as increasingly complex algorithms enter high-stakes domains such as healthcare, finance, and autonomous vehicles. This thesis examines Tim Miller’s argument that insights from social science—specifically from philosophy, psychology, and cognitive studies—can guide the design of more human-centered AI explanations. Conventional approaches in XAI often focus on technical or statistical transparency, yet Miller’s work demonstrates that individuals look for explanations framed around meaningful causes and contrasting alternatives. His analysis highlights how people naturally interpret unexpected events by focusing on abnormal conditions, using conversational shortcuts, and forming explanations grounded in beliefs, desires, and intentions. Applying these findings, this thesis discusses why a purely probabilistic rationale rarely satisfies users’ explanatory needs and how AI systems could instead offer contrastive answers and context-aware responses. Through a critical review, the thesis positions Miller’s perspective as a blueprint for integrating causal structures, social cues, and iterative dialogue into AI explanation mechanisms. 

## Table of Contents  
1. [Introduction and Overview](#introduction-and-overview)  
2. [Philosophical Foundations](#philosophical-foundations)  
   2.1. Philosophical Foundations and the Contrastive Nature of Explanation  
   2.2. Why Contrast Matters  
   2.3. A Kantian Lens on Explanation and Autonomy  
3. [Social Attribution and Intentionality](#social-attribution-and-intentionality)  
   3.1. Heider’s Legacy and the Birth of Social Attribution  
   3.2. Moving Shapes, Human Minds  
   3.3. Malle’s Framework for Explaining Behavior  
   3.4. Intentional Behavior “Gets” Priority  
4. [Cognitive Biases in Explanation](#cognitive-biases-in-explanation)  
   4.1. Abnormality (Abnormal Conditions Focus Model)  
   4.2. Mutability and Counterfactual Thinking  
   4.3. Inherent vs. Extrinsic Features (Inherence Bias)  
5. [Explanation as Dialogue](#explanation-as-dialogue)  
   5.1. Explanation Is Conversation, Not Just Attribution  
   5.2. Walton’s Dialogue Models  
   5.3. Designing AI for Dialogic Explanation  
6. [Primary Contribution and Analysis of Related Approaches](#primary-contribution-and-analysis-of-related-approaches)  
7. [Discussion, Conclusion and Future Research Directions](#discussion-conclusion-and-future-research-directions)  
8. [Appendix: Dialogic-Explanation Design](#appendix-dialogic-explanation-design) 




## Citing This Work  
If you use this thesis in your research, please cite it as:  
&gt; Hagi, N. (April 2025). *Explanation in Artificial Intelligence: Insights from the Social Sciences* (Bachelor’s thesis). 

## License  
This repository is licensed under the [Creative Commons Attribution 4.0 International License](LICENSE).  

---

*Questions or feedback? Feel free to file an issue or reach out to the author.*
